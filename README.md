# Beyond Message Passing: Neural Graph Pattern Machine

<div>

[![pytorch](https://img.shields.io/badge/PyTorch_2.4+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/get-started/locally/)
[![pyg](https://img.shields.io/badge/PyG_2.6+-3C2179?logo=pyg&logoColor=#3C2179)](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html)
![license](https://img.shields.io/badge/License-MIT-green.svg?labelColor=gray)
[![GPM arxiv](http://img.shields.io/badge/arxiv-2501.18739-yellow.svg)](https://arxiv.org/abs/2501.18739)



<img src="assets/logo.png" width='300'>
</div>

The official implementation of [Beyond Message Passing: Neural Graph Pattern Machine](https://arxiv.org/abs/2501.18739), ICML 25. We propose GPM, a path towards next-generation graph learning backbone. The logo is generated by DALLÂ·E 3.

Authored by [Zehong Wang](https://zehong-wang.github.io/), [Zheyuan Zhang](https://jasonzhangzy1757.github.io/), [Tianyi Ma](https://tianyi-billy-ma.github.io/), [Nitesh V Chawla](https://niteshchawla.nd.edu/), [Chuxu Zhang](https://chuxuzhang.github.io/), and [Yanfang Ye](http://yes-lab.org/).

Please contact `zwang43@nd.edu` or open an issue if you have questions.

## Overview

Most existing GNNs rely on message passing, which aggregates local neighborhood information iteratively and struggles to explicitly capture such fundamental motifs, like triangles, k-cliques, and rings. This limitation hinders both expressiveness and long-range dependency modeling. To this end, we introduce the Neural Graph Pattern Machine (GPM), a novel framework that bypasses message passing by learning directly from graph substructures. GPM efficiently extracts, encodes, and prioritizes task-relevant graph patterns, offering greater expressivity and improved ability to capture long-range dependencies. 


<img src="assets/workflow.svg">

<img src="assets/framework.svg">


**The workflow of Neural Graph Pattern Machine (GPM).** Given a graph dataset, GPM utilizes a random walk tokenizer to extract a set of patterns representing the learning instances (nodes, edges, or graphs). These patterns are first encoded by a sequential model and then processed by a transformer encoder, which identifies the dominant patterns relevant to downstream tasks.




## Installation

You may use conda to install the environment. Please run the following script. We run all experiments on a single A40 48G GPU, yet A GPU with 24G memory is sufficient to handle all datasets by setting smaller batch size. 

```
conda env create -f environment.yml
conda activate GPM
pip install  dgl -f https://data.dgl.ai/wheels/torch-2.4/cu121/repo.html
pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html
```


## Quick Start

The code of GPM is presented in folder `/GPM`. You can run `main.py` and specify any dataset to run experiments. To ensure reproducability, we provide hyper-parameters in `config/main.yaml`. You can simply use command `--use_params` to set tuned hyper-parameters. 

For example, you may use the command

```
python main.py --dataset computers --use_params
```

to reproduce the experimental results. 

### Dataset

In our paper, we use various graph benchmark datasets covering node classification, link prediction, graph classification/regression. You can indicate these datasets by 

- **Node Classification:** `cora_full`, `computers`, `arxiv`, `products`, `wikics`, `deezer`, `blog`, `flickr`, `flickr_small`. 

- **Link Prediction**: `link-cora`, `link-pubmed`, `link-collab`. 

- **Graph Classification**: `imdb-b`, `collab`, `reddit-m5k`, `reddit-m12k`. 

- **Graph Regression**: `zinc`, `zinc_full`.

We also provide the interfaces of other widely used datasets like `photo`, `physics`, `reddit`, etc. Please check the datasets in `GPM/data/pyg_data_loader.py` for details. 

## Customize Hyper-parameters

TODO

## Domain Adaptation

TODO

## Ablation

TODO

## Citation

If you find the repo is useful for your research, please cite the original paper properly.

```bibtex

@inproceedings{wang2024gpm,
  title={Beyond Message Passing: Neural Graph Pattern Machine},
  author={Wang, Zehong and Zhang, Zheyuan and Ma, Tianyi and Chawla, Nitesh V and Zhang, Chuxu and Ye, Yanfang},
  booktitle={Forty-Second International Conference on Machine Learning},
  year={2025}, 
}

@article{wang2025neural,
  title={Neural Graph Pattern Machine},
  author={Wang, Zehong and Zhang, Zheyuan and Ma, Tianyi and Chawla, Nitesh V and Zhang, Chuxu and Ye, Yanfang},
  journal={arXiv preprint arXiv:2501.18739},
  year={2025}
}

```
## Acknowledgement

This repository is based on the codebase of [PyG](https://github.com/pyg-team/pytorch_geometric), [OGB](https://github.com/snap-stanford/ogb), and [VQ](https://github.com/lucidrains/vector-quantize-pytorch). Thanks for their sharing!